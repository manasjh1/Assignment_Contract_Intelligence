ğŸ“„ Contract Intelligence APIğŸš€ OverviewThe Contract Intelligence API is a high-performance, asynchronous backend designed to analyze legal documents automatically. It leverages Large Language Models (Llama 3 via Groq) and Vector Search (Pinecone) to extract metadata, audit risks, and answer natural language questions about uploaded contracts.This project was specifically engineered for resource-constrained environments (512MB RAM), utilizing advanced memory management techniques, streaming batch processing, and ONNX-quantized embeddings to ensure enterprise-grade stability on free-tier infrastructure.ğŸ— System ArchitectureThe system follows an event-driven RAG (Retrieval-Augmented Generation) pipeline optimized for memory efficiency:Ingestion Layer:Streaming Uploads: Handles large PDF binaries via shutil streaming to avoid memory spikes.Batch Processing: Implements a strict 5-page batching strategy. The system processes a small window of pages, uploads vectors, and immediately invokes Python's gc.collect() to release memory before processing the next batch.Embedding Layer:Model: BAAI/bge-small-en-v1.5 (Quantized ONNX).Optimization: Runs on ONNX Runtime with OMP_NUM_THREADS=1 to minimize CPU contention and memory footprint.Retrieval & Generation:Vector Store: Pinecone Serverless (Cosine Similarity).LLM: Llama-3-70b-Versatile (via Groq) for near-instant inference (~300 tokens/sec).Operational Resilience:Lazy Loading: Heavy ML components (Embeddings, LLM Clients) are initialized only upon the first API request, preventing "Cold Start" timeouts.Automated Keep-Alive: A GitHub Actions workflow pings the health endpoint every 14 minutes to prevent the serverless container from idling.âš¡ï¸ Key FeaturesğŸ“„ Smart Ingestion: robustly handles large multi-page contracts without OOM (Out-of-Memory) crashes using a proprietary batch-and-GC strategy.ğŸ” Semantic Search: Context-aware Q&A that cites specific contract clauses (e.g., "What is the indemnity cap?").ğŸ›¡ Automated Audit: Instantly scans contracts for high-risk terms (termination, liability, auto-renewal) and generates a risk report.ğŸ“Š Structured Extraction: extract entities (parties, dates, amounts) into strict JSON formats using LLM function calling.ğŸ¤– CI/CD Automation: Fully automated deployment pipeline with self-healing keep-alive mechanisms.ğŸ›  Tech StackComponentTechnologyReason for ChoiceFrameworkFastAPIHigh-performance, native async support, and auto-generated Swagger UI.LLMGroq (Llama 3)Fastest inference speed on the market, critical for real-time chat.Vector DBPineconeServerless architecture eliminates database management overhead.EmbeddingsFastEmbedCPU-optimized, lightweight (<200MB RAM) alternative to standard PyTorch models.PDF EnginePyMuPDF (Fitz)C++ based parsing offers 10x speed improvement over PyPDF2.âš™ï¸ Performance EngineeringTo deploy this ML-heavy application on a Free Tier (512MB RAM) instance, specific optimizations were implemented:Quantized Embeddings: Replaced all-MiniLM-L6-v2 (PyTorch) with BAAI/bge-small-en-v1.5 (ONNX), reducing model memory usage by ~60%.Lazy Initialization Pattern: Global variables for VectorStore and LLM are initialized inside a singleton pattern (initialize_rag), ensuring the server boots instantly (<1s).Aggressive Garbage Collection: Intermediate objects (chunks, splitters) are explicitly deleted (del) and garbage collected (gc.collect()) after every 5-page batch to maintain a flat memory profile during large file uploads.ğŸš€ Getting StartedPrerequisitesPython 3.10+Pinecone API KeyGroq API Key1. Clone the Repositorygit clone [https://github.com/your-username/Assignment_Contract_Intelligence.git](https://github.com/your-username/Assignment_Contract_Intelligence.git)
cd Assignment_Contract_Intelligence
2. Install Dependenciespip install -r requirements.txt
3. Environment ConfigurationCreate a .env file in the root directory:GROQ_API_KEY=gsk_...
PINECONE_API_KEY=pc_...
PINECONE_INDEX=contract-intelligence
EMBEDDING_MODEL=BAAI/bge-small-en-v1.5
4. Run Locallyuvicorn main:app --reload --port 8000
Visit http://localhost:8000/docs to interact with the API Swagger UI.ğŸ“š API EndpointsMethodEndpointDescriptionPOST/ingestUpload PDF contracts (Streaming Batch Process).POST/askQ&A with RAG context and source citations.GET/ask/streamStreaming response for long-form answers.POST/auditRisk assessment against standard legal clauses.POST/extractStructured JSON extraction of metadata.GET/healthzHealth check probe for keep-alive bots.Deployment Configuration (Render)Runtime: Python 3Build Command: pip install -r requirements.txtStart Command: uvicorn main:app --host 0.0.0.0 --port 10000 --workers 1Environment Variables:OMP_NUM_THREADS=1MKL_NUM_THREADS=1FASTEMBED_CACHE_PATH=/opt/render/project/src/fastembed_cacheAuthorManas JhaBackend Engineer | AI Systems
